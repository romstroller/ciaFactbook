{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from FileTools import FileTools\n",
    "# from pprint import pprint\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "#import random\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# authenticate with API\n",
    "\n",
    "currWorkDir = os.getcwd()\n",
    "userDir = Path.home()\n",
    "keyPath = f\"{userDir}\\\\PYC\\\\ADMIN\\\\kaggle.json\"\n",
    "\n",
    "with open( keyPath, 'r' ) as f: keyDict = json.load( f )\n",
    "userTitle, keyTitle = keyDict.keys()\n",
    "kaggleUsername, kaggleKey = keyDict[ userTitle ], keyDict[ keyTitle ]\n",
    "\n",
    "os.environ[ 'KAGGLE_USERNAME' ] = kaggleUsername\n",
    "os.environ[ 'KAGGLE_KEY' ] = kaggleKey\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# retrieve dataset\n",
    "datasetOwner = 'lucafrance'\n",
    "datasetName = 'the-world-factbook-by-cia'\n",
    "api.dataset_download_files( f'{datasetOwner}/{datasetName}', path=\".\" )\n",
    "\n",
    "# await download\n",
    "fTools = FileTools()\n",
    "datasetFName = None\n",
    "print( \"Waiting for dataset download\" )\n",
    "while True:\n",
    "    time.sleep( 1 )\n",
    "    sortedFs = fTools.datesortFiles( currWorkDir, datasetName )\n",
    "    if len( sortedFs ) == 0: continue\n",
    "    datasetFName = list( sortedFs )[ 0 ]\n",
    "    print( f\"Latest: {datasetFName}\" )\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# extract and identify datafiles\n",
    "\n",
    "origDataDir = f\"{currWorkDir}\\\\data_or\"\n",
    "if not os.path.exists( origDataDir ): os.makedirs( origDataDir )\n",
    "\n",
    "if datasetFName and Path( datasetFName ).suffix == \".zip\":\n",
    "    with ZipFile( datasetFName, 'r' ) as zipF: zipF.extractall( origDataDir )\n",
    "\n",
    "dataPaths = [ f\"{origDataDir}\\\\{pth}\" for pth in os.listdir( origDataDir )\n",
    "    if Path( pth ).suffix == \".csv\" ]\n",
    "\n",
    "if len( dataPaths )>0:\n",
    "    dffBook = pd.read_csv( [ pth for pth in dataPaths ][ 0 ] )\n",
    "    print( \"Got dffBook DF from extracted dataset at:\\n\", dataPaths[ 0 ] )\n",
    "else: print( \"Failed get CSV\" ); sys.exit()\n",
    "\n",
    "dffBook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Analyse cell data for numbers and units\n",
    "\n",
    "def matchNumbers( df_in, coIdex, patrn ):\n",
    "    # get any number-pattern match from each row in a list\n",
    "    return df_in.iloc[ :, coIdex ].str.findall( patrn )\n",
    "\n",
    "\n",
    "def getRemainder( df_in, coIdex, pattrn ):\n",
    "    # store non-number remainder of string (potential unit etc)\n",
    "    return df_in.iloc[ :, coIdex ].str.replace( pattrn, '' )\n",
    "\n",
    "\n",
    "# MATCHING NUMBERS\n",
    "# REGEX:\n",
    "#   capture group             (                 \n",
    "#   zero/one                  [+-]?             possible number sign\n",
    "#   1-3 nums                  \\d{1,3}           up to three straight nums\n",
    "#   non-capture subgroup      (?:               possible thousand-groups\n",
    "#     comma and 3 nums          ,\\d{3}          (sep. comma)\n",
    "#     zero or more times        )*              \n",
    "#   non-capture subgroup      (?:               then possible decimal part\n",
    "#     decimal and 1+nums        \\.\\d+           \n",
    "#     zero/one time             )?            \n",
    "#   OR (alt. to last seq)     |                 or no groups, just              \n",
    "#     0+ nums, dec, 1+nums      \\d*\\.\\d+        more nums and poss decimal\n",
    "#   OR (alt. to last seq)     |             \n",
    "#     1+ nums                   \\d+             or just more numbers.\n",
    "#   Close capture group       )\n",
    "#   ( match basic number last to capture greatest valid str segment )\n",
    "\n",
    "patt = re.compile( r'([+-]?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d*\\.\\d+|\\d+)' )\n",
    "\n",
    "dfFbDict = { }  # to collect column data during cleaning\n",
    "checkTypes = { }\n",
    "colDex = 1\n",
    "for colName in dffBook.columns[ colDex: ]:\n",
    "    origCol = dffBook.iloc[ :, colDex ]\n",
    "    # get match if string, store if already float, catch unexpected\n",
    "    if type( origCol[ 0 ] ) == str: colDict = {\n",
    "        'matchedNums': matchNumbers( dffBook, colDex, patt ),\n",
    "        'remainder': getRemainder( dffBook, colDex, patt ) }\n",
    "    elif type( origCol[ 0 ] ) == np.float64: colDict = {\n",
    "        'matchedNums': origCol.astype( float ), 'remainder': [ ] }\n",
    "    elif type( origCol[ 0 ] ) != float:\n",
    "        checkTypes[ type( origCol[ 0 ] ) ] = colDex\n",
    "        colDict = { 'matchedNums': origCol, 'remainder': [ ] }\n",
    "    else: colDict = {\n",
    "        'matchedNums': matchNumbers( dffBook, colDex, patt ),\n",
    "        'remainder': getRemainder( dffBook, colDex, patt ) }\n",
    "    colDict[ 'origCol' ] = origCol\n",
    "    dfFbDict[ colName ] = colDict\n",
    "    colDex += 1\n",
    "\n",
    "for i in checkTypes: print( f\"unexpected: {i} at {checkTypes[ i ]}\" )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split first match item from list as float to \"clean\", store else to \"split\"\n",
    "def splitFirstOther( matchList ):\n",
    "    firstVals, splitVals, checkVals = [ ], [ ], [ ]\n",
    "    for mNum in range( len( matchList ) ):\n",
    "        el = matchList[ mNum ]\n",
    "        isFilldList = (type( el ) == list) and (len( el )>0)\n",
    "        if isFilldList:  # remove any thousandcomma to support convert\n",
    "            firstVals.append( float( ''.join( el[ 0 ].split( ',' ) ) ) )\n",
    "            splitVals.append( [ v for v in el[ 1: ] ] )\n",
    "        else:  # check all else are either nan or empty matchlist\n",
    "            if ((type( el ) == list and len( el )>0) and\n",
    "                (type( el ) != list and math.isnan( el ) == False)):\n",
    "                checkVals.append( el )\n",
    "            firstVals.append( np.nan )\n",
    "            splitVals.append( np.nan )\n",
    "    return firstVals, splitVals, checkVals\n",
    "\n",
    "\n",
    "for colName in dfFbDict:\n",
    "    colDict = dfFbDict[ colName ]\n",
    "    (colDict[ 'clean' ],\n",
    "    colDict[ 'splitVals' ],\n",
    "    colDict[ 'checkVals' ]) = splitFirstOther( colDict[ 'matchedNums' ] )\n",
    "\n",
    "# Raise message if got uncategorized data\n",
    "for colName in dfFbDict:\n",
    "    if len( dfFbDict[ colName ][ 'checkVals' ] )>0:\n",
    "        print( f\"Got checkvals for {colName}\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dictionary columns to DF, checking is now float\n",
    "newCols = [ ]\n",
    "dfFloat = dffBook.iloc[ :, 0 ]  # start with countries\n",
    "for colName in dfFbDict:\n",
    "    clean = pd.Series( dfFbDict[ colName ][ 'clean' ] )\n",
    "    lenFloat = len( [ i for i in clean if type( i ) == float ] )\n",
    "    if lenFloat>len( clean ) * 0.90:\n",
    "        newCols.append( colName )\n",
    "        dfFloat = pd.concat( [ dfFloat, clean ], axis=1 )\n",
    "    else: print( \"col is less than 90% float. Dropping...\" )\n",
    "\n",
    "dfFloat.columns = [ 'Country' ] + newCols\n",
    "dfFloat\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Enforce non-nan threshold for rows and columns\n",
    "\n",
    "def nanThreshold( notNan ):  # average plus .5 standard deviation (rounded)\n",
    "    return int( (sum( notNan ) / len( notNan )) + 0.5 * np.std( notNan ) )\n",
    "\n",
    "\n",
    "def nonNanFromDims( dfr, dim = 1 ):\n",
    "    nonNans = [ ]\n",
    "    for pos in range( 0, dfr.shape[ dim ] ):\n",
    "        vals = (dfr.iloc[ :, pos ].tolist() if dim == 1\n",
    "                else dfr.loc[ [ pos ] ].values.tolist()[ 0 ])\n",
    "        \n",
    "        nonNans.append( [ vals, len( [ v for v in vals\n",
    "            if (type( v ) == float and not math.isnan( v )) ] ) ] )\n",
    "    \n",
    "    _thresh = nanThreshold( [ nval for _, nval in nonNans ] )\n",
    "    keepVals = [ kval for kval, nnul in nonNans if nnul>=_thresh ]\n",
    "    print( f\"non-nan[ {len( keepVals )} ] thr[ {_thresh} ] dim[ {dim} ]\" )\n",
    "    \n",
    "    return keepVals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ROWS\n",
    "dfRowsClean = pd.DataFrame( nonNanFromDims( dfFloat, dim=0 ) )\n",
    "\n",
    "# add a columnindex row to track names of kept columns\n",
    "dfRowsClean.loc[ -1 ] = dfFloat.columns\n",
    "dfRowsClean.index = dfRowsClean.index + 1\n",
    "dfRowsClean.sort_index( inplace=True )\n",
    "dfRowsClean\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# COLS\n",
    "keepCols = nonNanFromDims( dfRowsClean, dim=1 )\n",
    "\n",
    "# convert to numeric df\n",
    "dfColsClean = pd.DataFrame( { col[ 0 ]: col[ 1: ] for col in keepCols } )\n",
    "df = dfColsClean.apply( pd.to_numeric, errors='ignore' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# final type check & add countries\n",
    "dTypeDict = dfColsClean.dtypes\n",
    "for itemName in list( dTypeDict.keys() ):\n",
    "    if dTypeDict[ itemName ] != np.float64: print( f\"NOT FLOAT: {itemName}\" )\n",
    "dfColsClean.insert( 0, 'Country', dfRowsClean.iloc[ :, 0 ].tolist()[ 1: ] )\n",
    "dfColsClean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Review clean DF\n",
    "fbIsNa = dffBook.isna().sum().sum()\n",
    "dfIsNa = dfColsClean.isna().sum().sum()\n",
    "fbDim = dffBook.shape[ 0 ] * dffBook.shape[ 1 ]\n",
    "dfDim = dfColsClean.shape[ 0 ] * dfColsClean.shape[ 1 ]\n",
    "print( f\"factbook originally shape: {dffBook.shape}\" )\n",
    "print( f\"    NAN-density: {(fbIsNa / fbDim):.2f}% \"\n",
    "       f\"({fbIsNa} NaN in {fbDim})\" )\n",
    "print( f\"clean dataframe has shape: {dfColsClean.shape}\" )\n",
    "print( f\"    NAN-density: {(dfIsNa / dfDim):.2f}% \"\n",
    "       f\"({dfIsNa} NaN in {dfDim})\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reporting to identify any differences of unit scale within features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add some further cleaning to remainder to reduce non-unit variations\n",
    "\n",
    "cleanReman = { }\n",
    "\n",
    "cleanPatts = [\n",
    "    r'(\\([^)]*\\))',  # remove all bracketed\n",
    "    r'(.* m)$|(.* m) '  # remove all before \" m[endline/space]\" (mtn names)\n",
    "    ]\n",
    "\n",
    "for colName in list( dfColsClean.columns )[ 1: ]:\n",
    "    remainder = dfFbDict[ colName ][ 'remainder' ]\n",
    "    for pattStr in cleanPatts:\n",
    "        patt = re.compile( pattStr )\n",
    "        remainder = remainder.str.replace( patt, '' )\n",
    "    cleanReman.update( { colName: remainder } )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CHECK FOR SCALE FACTORS\n",
    "\n",
    "def runScaleAnalysis( dfr, remDict ):\n",
    "    colList = list( dfr.columns )\n",
    "    dropFeatrs = [ ]\n",
    "    cleanNotes = { }\n",
    "    \n",
    "    for pos in range( 1, len( colList ) ):\n",
    "        colNam = colList[ pos ]\n",
    "        colSeg = dfr.iloc[ :, pos ].tolist()[ :10 ]\n",
    "        remndr = set( remDict[ colNam ] )\n",
    "        rMainPrint = \"\"\n",
    "        for r in list( remndr )[ :25 ]:\n",
    "            if type( r ) == float: rMainPrint = rMainPrint + f\"{r}\\n\"\n",
    "            else: rMainPrint = rMainPrint + f\"{r[ :60 ]}\\n\"\n",
    "        \n",
    "        report = (\n",
    "            f\"COL [ {pos} ] {colNam}\\n\\n\"\n",
    "            f\"CLEANVALS:\\n{colSeg}\\n\\n\"\n",
    "            f\"REMAINDER (unq in col: {len( remndr )}):\\n{rMainPrint}\\n\")\n",
    "        \n",
    "        report_a = report + \"\\nACCEPT(A), BREAK(B), CLEAN/SCALE NOTE(C), DROP(D)\"\n",
    "        report_b = report_a + \"\\n\\nPLEASE MAKE A SELECTION:\\n\\n\"\n",
    "        usinp = input( report_a )\n",
    "        while usinp not in [ 'a', 'd', 'c', 'b' ]: usinp = input( report_b )\n",
    "        if usinp == 'b': break\n",
    "        elif usinp == 'a': continue\n",
    "        elif usinp == 'd': dropFeatrs.append( colNam )\n",
    "        else: cleanNotes.update( { colNam:\n",
    "            input( f\"{report[ :250 ]}...\\n\\n\\nCLEANING/SCALE NOTE\" ) } )\n",
    "    \n",
    "    return dropFeatrs, cleanNotes\n",
    "\n",
    "# dropFeatrs, cleanNotes = runScaleAnalysis(dfColsClean, cleanReman)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # SAVE SCALE ANALYSIS DATA\n",
    "# stmp = fTools.dtStamp()\n",
    "# fTools.storePKL( dropFeatrs, f'dropFeatrs_{stmp}', currWorkDir, subdir=None )\n",
    "# fTools.storePKL( cleanNotes, f'cleanNotes_{stmp}', currWorkDir, subdir=None )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LOAD SCALE ANALYSIS DATA\n",
    "pklFiles = [ fi for fi in [ open( pth, 'rb' )\n",
    "    for pth in [ list( dKey )[ 0 ]\n",
    "        for dKey in [ fTools.datesortFiles( currWorkDir, fNam )\n",
    "            for fNam in [ 'dropFeatrs', 'cleanNotes' ]\n",
    "            ] ] ] ]\n",
    "\n",
    "dropFeats, scaleNotes = [ pickle.load( fi ) for fi in pklFiles ]\n",
    "for fi in pklFiles: fi.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = dfColsClean.copy()\n",
    "df.drop( dropFeats, axis=1, inplace=True )\n",
    "scaleKeys = [ dkey for dkey in scaleNotes if dkey not in dropFeats ]\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# update value by matching remainder scale fragment via country reference\n",
    "\n",
    "dct = dfFbDict.copy()\n",
    "\n",
    "scaleDict = {\n",
    "    \"million\": 1000000,\n",
    "    \"billion\": 1000000000,\n",
    "    \"trillion\": 1000000000000 }\n",
    "\n",
    "cleanCountries = list( df[ 'Country' ] )\n",
    "\n",
    "for colName in scaleKeys:\n",
    "    colVals = [ ]\n",
    "    row = 0\n",
    "    \n",
    "    # checking remnantcol (HAS PRE-CLEAN ENTRIES) for match\n",
    "    for remnt in dct[ colName ][ 'remainder' ]:\n",
    "        country = dffBook[ 'Country' ][ row ]\n",
    "        if country not in cleanCountries: row += 1; continue\n",
    "        val = df.loc[ df[ 'Country' ] == country ][ colName ].iloc[ 0 ]\n",
    "        if type( remnt ) == float: row += 1; colVals.append( val ); continue\n",
    "        \n",
    "        if remnt.startswith( \"-$\" ): val = 0 - val  # fix $ breaking neg float\n",
    "        \n",
    "        matches = [ ]\n",
    "        for scale in scaleDict:  # apply lowest-index matched scale\n",
    "            try: matches.append( [ remnt.index( scale ), scale ] )\n",
    "            except ValueError: continue\n",
    "        \n",
    "        if len( matches )>0:  # sort by lowest index (first val of match)\n",
    "            matchScale = sorted( matches, key=lambda x: x[ 0 ] )[ 0 ][ 1 ]\n",
    "            val = val * scaleDict[ matchScale ]\n",
    "        colVals.append( val )\n",
    "        row += 1\n",
    "    \n",
    "    df[ colName ] = colVals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a few top-tens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# need to go back and get units\n",
    "def showTopTen( featName, asc=False ):\n",
    "    print( featName )\n",
    "    \n",
    "    df10 = pd.concat( [ df[ 'Country' ], pd.Series( df[ featName ] ) ],\n",
    "        axis=1 ).sort_values( by=[ featName ], ascending=asc )[ :10 ]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.suptitle( \"TOP TEN:\\n\" + featName, fontsize=10 )\n",
    "    \n",
    "    ax = fig.add_axes( [ 0, 0, 1, 1 ] )\n",
    "    ax.bar( df10.iloc[ :, 0 ], df10.iloc[ :, 1 ] )\n",
    "    ax.set_xticklabels( labels=df10.iloc[ :, 0 ], rotation=45, ha='right' )\n",
    "    ax.ticklabel_format( axis='y', useOffset=False, style='plain' )\n",
    "    for tick in ax.xaxis.get_major_ticks(): tick.label.set_fontsize( 14 )\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# largest countries. \n",
    "showTopTen( 'Geography: Area - total' )\n",
    "# Some thoughts:\n",
    "#   The invasion of Ukraine should not be considered an act of claustrophobia\n",
    "#   Looking at sovereign territories as real-estate for future resources [..]\n",
    "#   ...\n",
    "\n",
    "# highest percent water area\n",
    "df[ 'Water-area ratio' ] = (\n",
    "    df[ 'Geography: Area - water' ] /\n",
    "    df[ 'Geography: Area - total' ])\n",
    "showTopTen( 'Water-area ratio' )\n",
    "\n",
    "# It should be safe to posit that Canada is likely at least somewhat well-known \n",
    "# as a destination for those people with metal detectors you always see a few \n",
    "# hundred meters away at the beach.\n",
    "showTopTen( 'Geography: Coastline' )\n",
    "\n",
    "# greatest height from lowest point\n",
    "# China may not have as high a mountain as Sagarmatha(?), but a hypothetical \n",
    "# mountain from its lowest to heighest elevation would dwarf it. \n",
    "df[ 'Elevation difference' ] = (\n",
    "    df[ 'Geography: Elevation - highest point' ] -\n",
    "    df[ 'Geography: Elevation - lowest point' ])\n",
    "showTopTen( 'Elevation difference' )\n",
    "\n",
    "# highest percent of population in the 65+ bracket (Japan by far)\n",
    "showTopTen( 'People and Society: Age structure - 65 years and over' )\n",
    "# Noting that, while Japan retains the lead, the difference is far less stark\n",
    "# in terms of median age\n",
    "showTopTen( 'People and Society: Median age - total' )\n",
    "\n",
    "# Ratio of irrigated land to total land\n",
    "df[ 'Irrigated-area ratio' ] = (\n",
    "    df[ 'Geography: Irrigated land' ] /\n",
    "    df[ 'Geography: Area - total' ])\n",
    "showTopTen( 'Irrigated-area ratio' )\n",
    "\n",
    "# Syria's far advancement in population growth must be linked to its even \n",
    "# greater leadership in net migration. What's the story there?\n",
    "showTopTen( 'People and Society: Population growth rate' )\n",
    "showTopTen( 'People and Society: Net migration rate' )\n",
    "# Surprised to see Aus as one of the T10 countries for net mig rate\n",
    "\n",
    "# UAE's men-to-women ratio is singularly, startlingly the most weighted to \n",
    "# the former in the in the 25-54 years, 55-64yo and 65+ brackets, while\n",
    "# all runners-up change position or drop from the top. What gives?\n",
    "showTopTen( 'People and Society: Sex ratio - 25-54 years' )\n",
    "showTopTen( 'People and Society: Sex ratio - 55-64 years' )\n",
    "# At the same time, has (by good measure) the highest value for\n",
    "# the percentage of total population that is 25-54 years old.\n",
    "showTopTen( 'People and Society: Age structure - 25-54 years' )\n",
    "# Interesting combination - purely speculating, one can imagine a dominating\n",
    "# social discourse might concern the relation of older men to working-age adults.\n",
    "# This consideration is furnished with the dependency ratio, in which\n",
    "# UAE is by very, very far the the world chart-topper:\n",
    "showTopTen( 'People and Society: Dependency ratios - potential support ratio' )\n",
    "# (What is the DR, and how would these three figures relate?)\n",
    "\n",
    "# People and Society: Current Health Expenditure\n",
    "#   US tops; surprised after hearing how much is copped by private citizens\n",
    "showTopTen( 'People and Society: Current Health Expenditure' )\n",
    "# The only country in top ten expenditure is in top ten physician density (Belg)\n",
    "showTopTen( 'People and Society: Physicians density' )\n",
    "# physician density strikes me as vital, as a population's relationship with\n",
    "#   health-seeking, at a preventative stage, [is tied] to its familiarity and \n",
    "#   access to responsive human consultation.\n",
    "\n",
    "# South Africa, and Southern Africa in general, has an enormous HIV problem.\n",
    "# showTopTen( 'People and Society: HIV/AIDS - people living with HIV/AIDS' )\n",
    "df['People living with HIV/AIDs as percentage of population'] = (\n",
    "    df[ 'People and Society: HIV/AIDS - people living with HIV/AIDS' ] /\n",
    "    df[ 'People and Society: Population' ])\n",
    "showTopTen( 'People living with HIV/AIDs as percentage of population' )\n",
    "\n",
    "# Nearly 50% of the people in Burma smoke\n",
    "showTopTen( 'People and Society: Tobacco use - total' )\n",
    "# However, while European nations remain in the top when limited to females,\n",
    "# the leading Asia-Pacific nations Burma, PNG and Indonesia disappear,\n",
    "# whereas for men they remain.\n",
    "# People and Society: Tobacco use - female\n",
    "# People and Society: Tobacco use - male\n",
    "\n",
    "# who has the highest combined score for both obesity prevalence and \n",
    "# children 4- underweight? \n",
    "# list top ten where both scores are above mean for feature\n",
    "\n",
    "# EDUCATION DISPARITIES:\n",
    "# People and Society: Education expenditures vs \n",
    "    # People and Society: Literacy - total population\n",
    "# People and Society: Literacy - male vs\n",
    "    # People and Society: Literacy - female\n",
    "    \n",
    "# Pat on the back for Aus: by a modest yet significant margin, longest\n",
    "#   \"school life expectancy\" (percent competing teriary?) total AND\n",
    "#   the position holds for women as much as men.\n",
    "\n",
    "# Environment: Air pollutants - carbon dioxide emissions\n",
    "#   China leads by twice its nearest competitor, the US. See coal-relation (hah)\n",
    "# Similar stats for Environment: Air pollutants - methane emission.\n",
    "\n",
    "# A pleasant pit-stop in Finnish Forests:\n",
    "# Environment: Land use - forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "listStart = 75\n",
    "showNumber = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"{listStart}:{listStart+showNumber}\")\n",
    "for i in list( df.columns )[ listStart:listStart+showNumber ]: showTopTen( i )\n",
    "listStart += showNumber\n",
    "\n",
    "# People and Society: Current Health Expenditure\n",
    "#   US tops; surprised after hearing how much is copped by private citizens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Things that China is in the top-ten/5/3/1 of\n",
    "# (Who else are the \"most top 10/5/3/1\" countries?)\n",
    "# combine bottom-tens somehow?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate CORRELATION DICTIONARY where keys are correlations,\n",
    "#   values are key-value pairs of baseCol : compareDict\n",
    "#   CompareDict key-value is col num : correlation coefficient\n",
    "\n",
    "# correlDict = { }\n",
    "# baseCol = 1\n",
    "# while True:\n",
    "#     colCorrs = { }\n",
    "#     for colPos in range( baseCol + 1, df.shape[ 1 ] ):\n",
    "#         colCorrs[ colPos ] = df.iloc[ :, baseCol ].corr( df.iloc[ :, colPos ] )\n",
    "#     correlDict[ baseCol ] = colCorrs\n",
    "#     baseCol += 1\n",
    "#     if baseCol == df.shape[ 1 ]: print( f\"Completed correlations\" ); break\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# move all most defs to import module, except where useful for process \n",
    "# communication\n",
    "# Heh. mosdef.\n",
    "\n",
    "# END"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}